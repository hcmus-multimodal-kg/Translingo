{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T12:17:59.448832Z",
     "iopub.status.busy": "2025-08-01T12:17:59.448585Z",
     "iopub.status.idle": "2025-08-01T12:18:04.799948Z",
     "shell.execute_reply": "2025-08-01T12:18:04.799112Z",
     "shell.execute_reply.started": "2025-08-01T12:17:59.448813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install evaluate sacrebleu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:20:02.903269Z",
     "iopub.status.busy": "2025-08-01T12:20:02.902464Z",
     "iopub.status.idle": "2025-08-01T12:20:02.907044Z",
     "shell.execute_reply": "2025-08-01T12:20:02.906292Z",
     "shell.execute_reply.started": "2025-08-01T12:20:02.903242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import tqdm as tqdm_\n",
    "from torch.optim import AdamW\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:22:53.849855Z",
     "iopub.status.busy": "2025-08-01T12:22:53.849581Z",
     "iopub.status.idle": "2025-08-01T12:23:07.976131Z",
     "shell.execute_reply": "2025-08-01T12:23:07.975521Z",
     "shell.execute_reply.started": "2025-08-01T12:22:53.849834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978000 / 2978000 / 2978000\n",
      "                                                  en  \\\n",
      "0  I knew if I wanted to live, I would have to th...   \n",
      "1  But once I lost my sight and was walking along...   \n",
      "2  She is a member of FRELIMO and was elected to ...   \n",
      "3                        What are we supposed to do?   \n",
      "4  And this is really what I want to talk to you ...   \n",
      "\n",
      "                                                  vi  \n",
      "0  Tôi biết nếu tôi muốn sống, tôi phải biết nghĩ...  \n",
      "1  Nhưng khi mất đi thị giác và đi dọc trên đường...  \n",
      "2  Cô là thành viên của FRELIMO và được bầu vào H...  \n",
      "3                           Gia đình tớ phải làm gì?  \n",
      "4  Đây là điều mà tôi thực sự muốn nói hôm nay - ...  \n",
      "18720 / 18720 / 18720\n",
      "                                                  en  \\\n",
      "0  Tanjirou Kamado is a kindhearted, intelligent ...   \n",
      "1  Incorporate gold and bronze into your look, es...   \n",
      "2  You're better off just getting to work despite...   \n",
      "3                           She's not Iris, I guess.   \n",
      "4  That way, you will not need to feel embarrasse...   \n",
      "\n",
      "                                                  vi  \n",
      "0  Tanjirou Kamado là một cậu bé tốt bụng, thông ...  \n",
      "1  Thêm màu vàng kim và màu đồng vào bộ trang phụ...  \n",
      "2  Tốt hơn bạn nên làm việc mặc kệ điều kiện hạn ...  \n",
      "3                    Có lẽ vì cô ấy không phải Iris.  \n",
      "4  Như vậy bạn sẽ không phải cảm thấy xấu hổ vì t...  \n",
      "19152 / 19152 / 19152\n",
      "                                                  en  \\\n",
      "0  They add value to a person's life but unlike o...   \n",
      "1    It gave the location of a major Energon source.   \n",
      "2  The short story, for example, people are sayin...   \n",
      "3  And countries in emerging markets do not need ...   \n",
      "4  Okay, and we have the means to create that kin...   \n",
      "\n",
      "                                                  vi  \n",
      "0  Chúng làm tăng thêm giá trị cho cuộc sống của ...  \n",
      "1  Nó cho biết vị chí của một nguồn Energon cực lớn.  \n",
      "2  Lấy ví dụ, truyện ngắn, mọi người đang nói rằn...  \n",
      "3  Và các nền kinh tế đang phát triển không cần t...  \n",
      "4  Được rồi, chúng ta đã có cơ sở để tạo ra một s...  \n"
     ]
    }
   ],
   "source": [
    "def load_dataset_kaggle(en_path, vi_path, sample_size=None):\n",
    "    en_df = pd.read_csv(en_path, names=[\"en\"], encoding=\"utf-8\")\n",
    "    vi_df = pd.read_csv(vi_path, names=[\"vi\"], encoding=\"utf-8\")\n",
    "    df = pd.concat([en_df, vi_df], axis=1).dropna().reset_index(drop=True)\n",
    "    print(len(en_df),\"/\",len(vi_df),\"/\",len(df))\n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# 🔹 Đường dẫn Kaggle\n",
    "base_path = \"/kaggle/input/translation-data\"\n",
    "\n",
    "# 🔹 Load training data\n",
    "df_training = load_dataset_kaggle(\n",
    "    f\"{base_path}/train_en.csv\",\n",
    "    f\"{base_path}/train_vi.csv\",\n",
    "    sample_size=80000\n",
    ")\n",
    "print(df_training.head())\n",
    "\n",
    "# 🔹 Load validation data\n",
    "df_validation = load_dataset_kaggle(\n",
    "    f\"{base_path}/dev_en.csv\",\n",
    "    f\"{base_path}/dev_vi.csv\",\n",
    "    sample_size=10000\n",
    ")\n",
    "print(df_validation.head())\n",
    "\n",
    "# 🔹 Load test data\n",
    "df_test = load_dataset_kaggle(\n",
    "    f\"{base_path}/test_en.csv\",\n",
    "    f\"{base_path}/test_vi.csv\",\n",
    "    sample_size=10000\n",
    ")\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:23:07.977406Z",
     "iopub.status.busy": "2025-08-01T12:23:07.977184Z",
     "iopub.status.idle": "2025-08-01T12:23:07.983081Z",
     "shell.execute_reply": "2025-08-01T12:23:07.982375Z",
     "shell.execute_reply.started": "2025-08-01T12:23:07.977389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = \"translate English to Vietnamese: \" + self.dataframe.loc[index, 'en']\n",
    "        tgt = self.dataframe.loc[index, 'vi']\n",
    "\n",
    "        src_tokenizer = self.tokenizer(src, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        tgt_tokenizer = self.tokenizer(tgt, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = src_tokenizer['input_ids'].squeeze()\n",
    "        attention_mask = src_tokenizer['attention_mask'].squeeze()\n",
    "        labels = tgt_tokenizer['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:23:07.984109Z",
     "iopub.status.busy": "2025-08-01T12:23:07.983892Z",
     "iopub.status.idle": "2025-08-01T16:24:52.619058Z",
     "shell.execute_reply": "2025-08-01T16:24:52.618258Z",
     "shell.execute_reply.started": "2025-08-01T12:23:07.984094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training:   0%|          | 0/2500 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Epoch 1/10 Training: 100%|██████████| 2500/2500 [23:03<00:00,  1.81it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.9839, Val Loss: 1.6297\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: 100%|██████████| 2500/2500 [23:09<00:00,  1.80it/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 1.6843, Val Loss: 1.4659\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 1.5564, Val Loss: 1.3583\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 1.4641, Val Loss: 1.2764\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.3929, Val Loss: 1.2150\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 1.3351, Val Loss: 1.1638\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 1.2863, Val Loss: 1.1201\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: 100%|██████████| 2500/2500 [23:09<00:00,  1.80it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 1.2441, Val Loss: 1.0815\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 1.2068, Val Loss: 1.0527\n",
      "✅ Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: 100%|██████████| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 1.1739, Val Loss: 1.0240\n",
      "✅ Saved best model.\n"
     ]
    }
   ],
   "source": [
    "#  Model setup\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "#  Dataloader setup\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(TranslationData(df_training, tokenizer, max_length), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TranslationData(df_validation, tokenizer, max_length), batch_size=batch_size)\n",
    "test_loader = DataLoader(TranslationData(df_test, tokenizer, max_length), batch_size=batch_size)\n",
    "\n",
    "#  Optimizer & Training\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "patience = 2\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "save_dir = \"/kaggle/working/my_t5_translation_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm_.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\n",
    "\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    #  Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(\" Saved best model.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⚠️ No improvement. Patience {epochs_no_improve}/{patience}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\" Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T16:24:52.620838Z",
     "iopub.status.busy": "2025-08-01T16:24:52.620615Z",
     "iopub.status.idle": "2025-08-01T16:33:56.266690Z",
     "shell.execute_reply": "2025-08-01T16:33:56.265926Z",
     "shell.execute_reply.started": "2025-08-01T16:24:52.620820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [09:02<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Final BLEU score: 39.34\n"
     ]
    }
   ],
   "source": [
    "#  Evaluation\n",
    "model = T5ForConditionalGeneration.from_pretrained(save_dir).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(save_dir)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm_.tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        labels[labels == -100] = tokenizer.pad_token_id\n",
    "        decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend([[ref] for ref in decoded_refs])\n",
    "\n",
    "bleu = corpus_bleu(predictions, references)\n",
    "print(f\" Final BLEU score: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7984472,
     "sourceId": 12635680,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
