{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T12:17:59.448832Z",
     "iopub.status.busy": "2025-08-01T12:17:59.448585Z",
     "iopub.status.idle": "2025-08-01T12:18:04.799948Z",
     "shell.execute_reply": "2025-08-01T12:18:04.799112Z",
     "shell.execute_reply.started": "2025-08-01T12:17:59.448813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install evaluate sacrebleu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:20:02.903269Z",
     "iopub.status.busy": "2025-08-01T12:20:02.902464Z",
     "iopub.status.idle": "2025-08-01T12:20:02.907044Z",
     "shell.execute_reply": "2025-08-01T12:20:02.906292Z",
     "shell.execute_reply.started": "2025-08-01T12:20:02.903242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import tqdm as tqdm_\n",
    "from torch.optim import AdamW\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:22:53.849855Z",
     "iopub.status.busy": "2025-08-01T12:22:53.849581Z",
     "iopub.status.idle": "2025-08-01T12:23:07.976131Z",
     "shell.execute_reply": "2025-08-01T12:23:07.975521Z",
     "shell.execute_reply.started": "2025-08-01T12:22:53.849834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978000 / 2978000 / 2978000\n",
      "                                                  en  \\\n",
      "0  I knew if I wanted to live, I would have to th...   \n",
      "1  But once I lost my sight and was walking along...   \n",
      "2  She is a member of FRELIMO and was elected to ...   \n",
      "3                        What are we supposed to do?   \n",
      "4  And this is really what I want to talk to you ...   \n",
      "\n",
      "                                                  vi  \n",
      "0  TÃ´i biáº¿t náº¿u tÃ´i muá»‘n sá»‘ng, tÃ´i pháº£i biáº¿t nghÄ©...  \n",
      "1  NhÆ°ng khi máº¥t Ä‘i thá»‹ giÃ¡c vÃ  Ä‘i dá»c trÃªn Ä‘Æ°á»ng...  \n",
      "2  CÃ´ lÃ  thÃ nh viÃªn cá»§a FRELIMO vÃ  Ä‘Æ°á»£c báº§u vÃ o H...  \n",
      "3                           Gia Ä‘Ã¬nh tá»› pháº£i lÃ m gÃ¬?  \n",
      "4  ÄÃ¢y lÃ  Ä‘iá»u mÃ  tÃ´i thá»±c sá»± muá»‘n nÃ³i hÃ´m nay - ...  \n",
      "18720 / 18720 / 18720\n",
      "                                                  en  \\\n",
      "0  Tanjirou Kamado is a kindhearted, intelligent ...   \n",
      "1  Incorporate gold and bronze into your look, es...   \n",
      "2  You're better off just getting to work despite...   \n",
      "3                           She's not Iris, I guess.   \n",
      "4  That way, you will not need to feel embarrasse...   \n",
      "\n",
      "                                                  vi  \n",
      "0  Tanjirou Kamado lÃ  má»™t cáº­u bÃ© tá»‘t bá»¥ng, thÃ´ng ...  \n",
      "1  ThÃªm mÃ u vÃ ng kim vÃ  mÃ u Ä‘á»“ng vÃ o bá»™ trang phá»¥...  \n",
      "2  Tá»‘t hÆ¡n báº¡n nÃªn lÃ m viá»‡c máº·c ká»‡ Ä‘iá»u kiá»‡n háº¡n ...  \n",
      "3                    CÃ³ láº½ vÃ¬ cÃ´ áº¥y khÃ´ng pháº£i Iris.  \n",
      "4  NhÆ° váº­y báº¡n sáº½ khÃ´ng pháº£i cáº£m tháº¥y xáº¥u há»• vÃ¬ t...  \n",
      "19152 / 19152 / 19152\n",
      "                                                  en  \\\n",
      "0  They add value to a person's life but unlike o...   \n",
      "1    It gave the location of a major Energon source.   \n",
      "2  The short story, for example, people are sayin...   \n",
      "3  And countries in emerging markets do not need ...   \n",
      "4  Okay, and we have the means to create that kin...   \n",
      "\n",
      "                                                  vi  \n",
      "0  ChÃºng lÃ m tÄƒng thÃªm giÃ¡ trá»‹ cho cuá»™c sá»‘ng cá»§a ...  \n",
      "1  NÃ³ cho biáº¿t vá»‹ chÃ­ cá»§a má»™t nguá»“n Energon cá»±c lá»›n.  \n",
      "2  Láº¥y vÃ­ dá»¥, truyá»‡n ngáº¯n, má»i ngÆ°á»i Ä‘ang nÃ³i ráº±n...  \n",
      "3  VÃ  cÃ¡c ná»n kinh táº¿ Ä‘ang phÃ¡t triá»ƒn khÃ´ng cáº§n t...  \n",
      "4  ÄÆ°á»£c rá»“i, chÃºng ta Ä‘Ã£ cÃ³ cÆ¡ sá»Ÿ Ä‘á»ƒ táº¡o ra má»™t s...  \n"
     ]
    }
   ],
   "source": [
    "def load_dataset_kaggle(en_path, vi_path, sample_size=None):\n",
    "    en_df = pd.read_csv(en_path, names=[\"en\"], encoding=\"utf-8\")\n",
    "    vi_df = pd.read_csv(vi_path, names=[\"vi\"], encoding=\"utf-8\")\n",
    "    df = pd.concat([en_df, vi_df], axis=1).dropna().reset_index(drop=True)\n",
    "    print(len(en_df),\"/\",len(vi_df),\"/\",len(df))\n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ğŸ”¹ ÄÆ°á»ng dáº«n Kaggle\n",
    "base_path = \"/kaggle/input/translation-data\"\n",
    "\n",
    "# ğŸ”¹ Load training data\n",
    "df_training = load_dataset_kaggle(\n",
    "    f\"{base_path}/train_en.csv\",\n",
    "    f\"{base_path}/train_vi.csv\",\n",
    "    sample_size=80000\n",
    ")\n",
    "print(df_training.head())\n",
    "\n",
    "# ğŸ”¹ Load validation data\n",
    "df_validation = load_dataset_kaggle(\n",
    "    f\"{base_path}/dev_en.csv\",\n",
    "    f\"{base_path}/dev_vi.csv\",\n",
    "    sample_size=10000\n",
    ")\n",
    "print(df_validation.head())\n",
    "\n",
    "# ğŸ”¹ Load test data\n",
    "df_test = load_dataset_kaggle(\n",
    "    f\"{base_path}/test_en.csv\",\n",
    "    f\"{base_path}/test_vi.csv\",\n",
    "    sample_size=10000\n",
    ")\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:23:07.977406Z",
     "iopub.status.busy": "2025-08-01T12:23:07.977184Z",
     "iopub.status.idle": "2025-08-01T12:23:07.983081Z",
     "shell.execute_reply": "2025-08-01T12:23:07.982375Z",
     "shell.execute_reply.started": "2025-08-01T12:23:07.977389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = \"translate English to Vietnamese: \" + self.dataframe.loc[index, 'en']\n",
    "        tgt = self.dataframe.loc[index, 'vi']\n",
    "\n",
    "        src_tokenizer = self.tokenizer(src, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        tgt_tokenizer = self.tokenizer(tgt, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = src_tokenizer['input_ids'].squeeze()\n",
    "        attention_mask = src_tokenizer['attention_mask'].squeeze()\n",
    "        labels = tgt_tokenizer['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T12:23:07.984109Z",
     "iopub.status.busy": "2025-08-01T12:23:07.983892Z",
     "iopub.status.idle": "2025-08-01T16:24:52.619058Z",
     "shell.execute_reply": "2025-08-01T16:24:52.618258Z",
     "shell.execute_reply.started": "2025-08-01T12:23:07.984094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training:   0%|          | 0/2500 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Epoch 1/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:03<00:00,  1.81it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.9839, Val Loss: 1.6297\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:09<00:00,  1.80it/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 1.6843, Val Loss: 1.4659\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 1.5564, Val Loss: 1.3583\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 1.4641, Val Loss: 1.2764\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.3929, Val Loss: 1.2150\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 1.3351, Val Loss: 1.1638\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 1.2863, Val Loss: 1.1201\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:09<00:00,  1.80it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 1.2441, Val Loss: 1.0815\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 1.2068, Val Loss: 1.0527\n",
      "âœ… Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [23:08<00:00,  1.80it/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 1.1739, Val Loss: 1.0240\n",
      "âœ… Saved best model.\n"
     ]
    }
   ],
   "source": [
    "#  Model setup\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "#  Dataloader setup\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(TranslationData(df_training, tokenizer, max_length), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TranslationData(df_validation, tokenizer, max_length), batch_size=batch_size)\n",
    "test_loader = DataLoader(TranslationData(df_test, tokenizer, max_length), batch_size=batch_size)\n",
    "\n",
    "#  Optimizer & Training\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "patience = 2\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "save_dir = \"/kaggle/working/my_t5_translation_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm_.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\n",
    "\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    #  Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(\" Saved best model.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"âš ï¸ No improvement. Patience {epochs_no_improve}/{patience}\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\" Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T16:24:52.620838Z",
     "iopub.status.busy": "2025-08-01T16:24:52.620615Z",
     "iopub.status.idle": "2025-08-01T16:33:56.266690Z",
     "shell.execute_reply": "2025-08-01T16:33:56.265926Z",
     "shell.execute_reply.started": "2025-08-01T16:24:52.620820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [09:02<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”µ Final BLEU score: 39.34\n"
     ]
    }
   ],
   "source": [
    "#  Evaluation\n",
    "model = T5ForConditionalGeneration.from_pretrained(save_dir).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(save_dir)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm_.tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        labels[labels == -100] = tokenizer.pad_token_id\n",
    "        decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend([[ref] for ref in decoded_refs])\n",
    "\n",
    "bleu = corpus_bleu(predictions, references)\n",
    "print(f\" Final BLEU score: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7984472,
     "sourceId": 12635680,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
